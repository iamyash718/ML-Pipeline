# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d2dw2gPyj8UotJ64hBmSUbyZtddwi3T_
"""

# 1. Install Kaggle and setup
!pip install -q kaggle

# 2. Create and write kaggle.json credentials from string
import os
kaggle_token = {
    "username":"your_kaggle_username_here",
    "key":"your_kaggle_api_key_here"
}

# Save to kaggle.json
os.makedirs("/root/.kaggle", exist_ok=True)
with open("/root/.kaggle/kaggle.json", "w") as f:
    import json
    json.dump(kaggle_token, f)

# 3. Set file permissions
!chmod 600 /root/.kaggle/kaggle.json

# 4. Download Kube-IDS0 Dataset from Kaggle
!kaggle datasets download -d redamorsli/kube-ids0

# 5. Unzip it into dataset folder
!unzip -q kube-ids0.zip -d dataset

# 6. Check files
print("Dataset downloaded and ready:")
!ls -R dataset

"""## **Data Loading and Inspection**"""

import pandas as pd

# path to main dataset file
data_path = "dataset/boa_dataset/processed/boa_dataset_ml_ready_frontend_microservice.csv"

# load data
df = pd.read_csv(data_path)

# show shape of dataset
print("Shape of dataset:", df.shape)

# show first 5 rows
print(df.head())

# show column names
print("Column names:", df.columns.tolist())

# check label column and value counts
label_column = None
for col in ["label", "class", "target", "attack_type"]:
    if col in df.columns:
        label_column = col
        break

if label_column:
    print(f"Label column found: {label_column}")
    print(df[label_column].value_counts())
else:
    print("No common label column found. Please check columns.")

"""## **Exploratory Data Analysis (EDA)**"""

import matplotlib.pyplot as plt
import seaborn as sns

# check missing values
missing_values = df.isnull().sum()
missing_total = missing_values.sum()
print("Total missing values in dataset:", missing_total)
print("Missing values per column (top 10):")
print(missing_values[missing_values > 0].sort_values(ascending=False).head(10))

# basic statistics
print("\nBasic statistics:")
print(df.describe().T.head(10))

# plot class distribution
plt.figure(figsize=(6,4))
sns.countplot(x="label", data=df, palette="viridis")
plt.title("Class Distribution")
plt.xlabel("Class Label")
plt.ylabel("Count")
plt.show()

"""## **Preprocessing and CNN + LSTM**"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
import numpy as np

# separate features and labels
X = df.drop(columns=["label"])
y = df["label"]

# one-hot encode labels
encoder = OneHotEncoder(sparse_output=False)
y_encoded = encoder.fit_transform(y.values.reshape(-1, 1))

# scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# split train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y
)

# reshape for CNN + LSTM (timesteps=1 for now, features=number of columns)
X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])

# print shapes
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""## **Building Hybrid model**"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv1D, LSTM, Dense, Dropout, LayerNormalization, MultiHeadAttention, Flatten
from tensorflow.keras.models import Model

# transformer encoder block
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # multi-head self attention
    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads)(inputs, inputs)
    x = Dropout(dropout)(x)
    x = LayerNormalization(epsilon=1e-6)(x)
    res = x + inputs

    # feed-forward network
    x = Dense(ff_dim, activation="relu")(res)
    x = Dropout(dropout)(x)
    x = Dense(inputs.shape[-1], activation="linear")(x)
    x = LayerNormalization(epsilon=1e-6)(x)
    return x + res

# model input
inp = Input(shape=(X_train.shape[1], X_train.shape[2]))

# CNN layer
x = Conv1D(filters=64, kernel_size=1, activation='relu')(inp)

# LSTM layer
x = LSTM(64, return_sequences=True)(x)

# Transformer block
x = transformer_encoder(x, head_size=64, num_heads=4, ff_dim=128, dropout=0.1)

# flatten and dense
x = Flatten()(x)
x = Dense(64, activation='relu')(x)
x = Dropout(0.3)(x)

# output layer
out = Dense(y_train.shape[1], activation='softmax')(x)

# model definition
model = Model(inputs=inp, outputs=out)

# compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# summary
model.summary()

"""# **Training and Evaluation CNN + LSTM**"""

# early stopping to avoid overfitting
from tensorflow.keras.callbacks import EarlyStopping

# callback
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# train model
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=20,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

# evaluate on test data
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import time

# predictions
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test, axis=1)

# classification report
print("Classification Report:")
print(classification_report(y_true, y_pred))

# confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.categories_[0], yticklabels=encoder.categories_[0])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

# inference latency measurement
start_time = time.time()
_ = model.predict(X_test, verbose=0)
end_time = time.time()
latency_per_sample = (end_time - start_time) / X_test.shape[0]
print(f"Average inference time per sample: {latency_per_sample*1000:.4f} ms")

"""### **Interpretability with Random Forest feature importance**"""

from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# create a feature extractor model (remove the final Dense softmax layer)
feature_extractor = Model(inputs=model.input, outputs=model.layers[-4].output)

# extract features for train and test sets
X_train_features = feature_extractor.predict(X_train, verbose=0)
X_test_features = feature_extractor.predict(X_test, verbose=0)

# flatten if needed
if len(X_train_features.shape) > 2:
    X_train_features = X_train_features.reshape(X_train_features.shape[0], -1)
    X_test_features = X_test_features.reshape(X_test_features.shape[0], -1)

# train a random forest on extracted features
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_features, np.argmax(y_train, axis=1))

# get feature importances
importances = rf.feature_importances_
indices = np.argsort(importances)[::-1]

# create table
importance_df = pd.DataFrame({
    'Feature_Index': indices,
    'Importance': importances[indices]
})

print("Top 10 Important Features from Deep Embeddings:")
print(importance_df.head(10))

# plot feature importances
plt.figure(figsize=(8,5))
plt.bar(range(10), importances[indices[:10]], align='center')
plt.xticks(range(10), indices[:10])
plt.xlabel('Embedding Feature Index')
plt.ylabel('Importance Score')
plt.title('Top 10 Deep Feature Importances')
plt.show()

"""## **Map Deep Embedding Features to Original Features**"""

# Train RF directly on original scaled features
rf_orig = RandomForestClassifier(n_estimators=100, random_state=42)
rf_orig.fit(X_train.reshape(X_train.shape[0], -1), np.argmax(y_train, axis=1))

# Get importance scores for original features
orig_importances = rf_orig.feature_importances_
orig_indices = np.argsort(orig_importances)[::-1]

# Create dataframe for top features
orig_importance_df = pd.DataFrame({
    'Original_Feature': X.columns[orig_indices],
    'Importance': orig_importances[orig_indices]
})

print("Top 10 Important Original Features:")
print(orig_importance_df.head(10))

# Plot
plt.figure(figsize=(10,5))
sns.barplot(x='Importance', y='Original_Feature', data=orig_importance_df.head(10), palette='viridis')
plt.title("Top 10 Important Original Network Traffic Features")
plt.show()